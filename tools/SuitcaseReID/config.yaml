DATA_ROOT: "/data1/zhaofanghan/OpenUnReID/datasets/"
LOGS_ROOT: "/data1/zhaofanghan/OpenUnReID/logs/"

MODEL:
  # architecture
  backbone: "resnet50"
  pooling: "gem"
  embed_feat: 0
  dropout: 0.

  dsbn: False # Set to False since we're only using one dataset

  sync_bn: True
  samples_per_bn: 16

  mean_net: False
  alpha: 0.999

  # pretraining
  imagenet_pretrained: True
  source_pretrained: null

DATA:
  height: 256
  width: 256 # Updated to square for better suitcase feature extraction
  norm_mean: [0.485, 0.456, 0.406]
  norm_std: [0.229, 0.224, 0.225]

  TRAIN:
    # Enhanced augmentation for single-view data
    is_autoaug: True # Enable auto augmentation

    is_flip: True
    flip_prob: 0.5

    is_pad: True
    pad_size: 10

    is_blur: True # Enable blur for more diverse views
    blur_prob: 0.5

    is_erase: True
    erase_prob: 0.7 # Increased for more diversity

    # dual augmentation for MMT
    is_mutual_transform: False
    mutual_times: 2

TRAIN:
  seed: 1
  deterministic: True
  # mixed precision training for PyTorch>=1.6
  amp: False

  # datasets
  datasets: { "suitcaseid": "trainval" }
  unsup_dataset_indexes: [0]

  epochs: 70 # More epochs for better clustering
  iters: 400

  LOSS:
    losses: { "hybrid_memory": 1. }
    temp: 0.05
    momentum: 0.2

  # validate
  val_dataset: "suitcaseid"
  val_freq: 5

  # sampler
  SAMPLER:
    num_instances: 4
    is_shuffle: True

  # data loader
  LOADER:
    samples_per_gpu: 16
    workers_per_gpu: 2

  # pseudo labels - adjusted for single-view suitcase data
  PSEUDO_LABELS:
    freq: 1 # epochs
    use_outliers: True
    norm_feat: True
    norm_center: True

    cluster: "dbscan"
    eps: [0.3, 0.4, 0.5] # Lower eps values to create more clusters
    min_samples: 2 # Reduced to form clusters more easily
    dist_metric: "jaccard"
    k1: 10 # Significantly reduced for much tighter matching
    k2: 3 # Reduced for tighter matching
    search_type: 0 # 0,1,2 for GPU, 3 for CPU (work for faiss)
    cluster_num: null

  # optim
  OPTIM:
    optim: "adam"
    lr: 0.00035
    weight_decay: 0.0005

  SCHEDULER:
    lr_scheduler: "single_step"
    stepsize: 30 # Adjusted for longer training
    gamma: 0.1

TEST:
  # datasets
  datasets: ["suitcaseid"]

  # data loader
  LOADER:
    samples_per_gpu: 32
    workers_per_gpu: 2

  # ranking setting
  dist_metric: "euclidean"
  norm_feat: True
  dist_cuda: True

  # post processing
  rerank: True # Enable reranking for better results
  search_type: 0 # 0,1,2 for GPU, 3 for CPU (work for faiss)
  k1: 20
  k2: 6
  lambda_value: 0.3
